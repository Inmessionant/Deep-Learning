## ########################################

####  调参经验

```
 - 学习率 3e-4
 - 数据归一化
 - L1 L2正则化 / weight decany
 - DropOut2d / DropPath / DropBlock[block size控制大小最好在7x7，；keep prob在整个训练过程中从1逐渐衰减到指定阈值
 - Batch Normalization / Group Normalization [每组channel为16]
 - BatchSize [大batchsize对小物体好，梯度累积]
 - OneCycleLR + SGD / Adam   (torch.optim.lr_scheduler.ReduceLROnPlateau)
 - Warm Up / Early stopping
 - 3x3卷积（有利于保持图像性质）
 - 卷积核权重初始化使用xavier（Tanh,Zé wéi'ěr）或者He normal（ReLU） 
 - cv2读取图片速度快比Pillow快
 - 加速训练pin_memory=true work_numbers=x(卡的数量x4) data.to(device,  no_blocking=True),设置为True后，数据直接保存在锁页内存中，后续直接传入cuda；否则需要先从虚拟内存中传入锁页内存中，再传入cuda
 - ReLU可使用inplace操作减少显存消耗
 - Focal Loss：对CE loss增加了一个调制系数来降低容易样本的权重值，使得训练过程更加关注困难样本
 - With Flooding:当training loss大于一个阈值时，进行正常的梯度下降；当training loss低于阈值时，会反过来进行梯度上升，让training loss保持在一个阈值附近，让模型持续进行"random walk"
 - Label Smoothing：使得原本的hard-target变为soft-target，让标签分布的熵增大,使网络优化更加平滑,通常用于减少训练的过拟合问题并进一步提高分类性能
```



#### 数据增强

```
 - Mix up / Cutout / Mosaic
 - Label Smoothing
 - 物体的复制粘贴（小物体）
 - 随机剪裁，翻转，缩放，亮度，色调，饱和度
 - 对普通数码照片进行归一化，可以简单的将0-255线性映射到0-1；而医学图像、遥感图像则不能简单的利用最小最大像元值归一化到0-1；
```

- 拼接增广指随机找几张图各取一部分或者缩小之后拼起来作为一幅图用，拼接出来的图有强烈的拼接痕迹；
- 抠洞指随机的将目标的一部分区域扣掉填充0值；

- **拼接、抠洞属于人为制造的伪显著区域，不符合实际情况，对工程应用来说没有意义，白白增加训练量；**
- **训练过程随机缩放也是没必要的，缩放之后的图像可能会导致特征图和输入图像映射错位；**



#### 深度学习挑战

- **应该更多地关注边缘情况（也就是异常值，或不寻常的情况），并思考这些异常值对预测可能意味着什么。**我们手上有大量的关于日常事务的数据，当前的技术很容易处理这些数据；而对于罕见的事件，我们得到的数据非常少，且目前的技术很难去处理这些数据；
- **我们人类拥有大量的不完全信息推理的技巧，也许可以克服生活中的长尾问题**。但对于目前流行的、更多依赖大数据而非推理的人工智能技术来说，长尾问题是一个非常严重的问题；
- 世上并不只有一种思维方式，因为思维并不是一个整体。相反，**思维是可以分为部分的，而且其不同部分以不同的方式运作**。例如，深度学习在识别物体方面做得相当不错，但在计划、阅读或语言理解方面做得差一些；
- **使用深度学习进行调试非常困难，因为没有人真正理解它是如何工作的，也没有人知道如何修复问题，**大众所知道的那种调试在经典编程环境中并不适用；



#### 有效阅读PyTorch源码

- **项目背景调研 + Paper**
- 阅读项目说明文档 + **README**
- 通过文件命名分析：**数据处理、数据加载**部分，通常命名xxx_dataloader.py等；**网络模型**构建部分，通常命名 resnet20.py model.py等；**训练部分**脚本，通常命名为train.py等；**测试部分**脚本，通常命名为test.py eval.py 等；**工具库**，通常命名为utils文件夹；
- **用IDE打开项目**，**找到项目运行的主入口**，阅读入口文件的逻辑，查看调用到了哪些 -  通过IDE的功能跳转到对应类或者函数进行继续阅读，配合代码注释进行分析。一开始可以泛读，大概了解整体流程，做一些代码注释，而后可以精读，找到文章的核心，反复理解核心实现；
- **3类BUG：**1.环境不兼容；深度学习框架；2.项目本身相关的BUG，这类BUG最好是在Issue区域进行查找，如果无法解决可以在issue部分详细描述自己的问题，等待项目库作者的解答；



#### 网络CheckList

- **从最简单的数据/模型开始，全流程走通：**
  - **模型简单**：解决一个深度学习任务，最好是先自己搭建一个最简单的神经网络；
  - **数据简单：** 一般来说少于**10个样本**做调试足够了，一定要**做过拟合测试** [如果你的模型无法在7、8个样本上过拟合，要么模型参数实在太少，要么有模型有BUG，要么数据有BUG，多选几个有代表性的输入数据有助于直接测试出非法数据格式；

- **选择合理的loss/评价指标，检查一下loss是否符合预期：**

  - **初始loss期望值和实际值误差是否过大：** 假如：CIFAR-10用Softmax Classifier进行10分类，那么一开始每个类别预测对的概率是0.1[随机预测]，Softmax loss使用的是negative log probability，所以正确的loss大概是：-ln（0.1）= 2.303左右；如果一开始loss不符合预期，那么可能是**模型初始化不均匀**或者**数据输入没有归一化；**

  - **多个loss相加，那这些loss的数值是否在同一个范围；**

  - **数据不均衡的时候尝试Focal Loss；**


- **网络中间输出、网络连接检查：**

  - 确认所有子网络的输入输出**Tensor对齐**，并确认全部都连接上了，可能有时候定义一个子网络，但放一边忘记连入主网络；

  - **梯度更新是否正确：** 如果某个参数没有梯度，那么是不是没有连入主网络； 有时候我们会通过参数名字来设置哪些梯度更新，哪些不更新，是否有误操作；

- **-时刻关注着模型参数：**

  - 所谓模型参数也就是一堆矩阵，如果这些数值中有些数值异常大/小，那么模型效果一般也会出现异常；


- **详细记录实验过程：**



## ########################################







## ########################################









## ########################################
