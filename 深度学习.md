

####  调参经验

```
 - 学习率 3e-4
 - 数据归一化
 - L1 L2正则化 / weight decany
 - DropOut2d / DropPath / DropBlock[block size控制大小最好在7x7，；keep prob在整个训练过程中从1逐渐衰减到指定阈值
 - Batch Normalization / Group Normalization [每组channel为16]
 - BatchSize [大batchsize对小物体好，梯度累积]
 - OneCycleLR + SGD / Adam   (torch.optim.lr_scheduler.ReduceLROnPlateau)
 - Warm Up / Early stopping
 - 3x3卷积（有利于保持图像性质）
 - 卷积核权重初始化使用xavier（Tanh,Zé wéi'ěr）或者He normal（ReLU） 
 - cv2读取图片速度快比Pillow快
 - 加速训练pin_memory=true work_numbers=x(卡的数量x4) data.to(device,  no_blocking=True),设置为True后，数据直接保存在锁页内存中，后续直接传入cuda；否则需要先从虚拟内存中传入锁页内存中，再传入cuda
 - ReLU可使用inplace操作减少显存消耗
 - Focal Loss：对CE loss增加了一个调制系数来降低容易样本的权重值，使得训练过程更加关注困难样本
 - With Flooding:当training loss大于一个阈值时，进行正常的梯度下降；当training loss低于阈值时，会反过来进行梯度上升，让training loss保持在一个阈值附近，让模型持续进行"random walk"
 - Label Smoothing：使得原本的hard-target变为soft-target，让标签分布的熵增大,使网络优化更加平滑,通常用于减少训练的过拟合问题并进一步提高分类性能
```



#### 数据增强

```
 - Mix up / Cutout / Mosaic
 - Label Smoothing
 - 物体的复制粘贴（小物体）
 - 随机剪裁，翻转，缩放，亮度，色调，饱和度
 - 对普通数码照片进行归一化，可以简单的将0-255线性映射到0-1；而医学图像、遥感图像则不能简单的利用最小最大像元值归一化到0-1；
```

- 拼接增广指随机找几张图各取一部分或者缩小之后拼起来作为一幅图用，拼接出来的图有强烈的拼接痕迹；
- 抠洞指随机的将目标的一部分区域扣掉填充0值；

- **拼接、抠洞属于人为制造的伪显著区域，不符合实际情况，对工程应用来说没有意义，白白增加训练量；**
- **训练过程随机缩放也是没必要的，缩放之后的图像可能会导致特征图和输入图像映射错位；**



#### 深度学习挑战

- **应该更多地关注边缘情况（也就是异常值，或不寻常的情况），并思考这些异常值对预测可能意味着什么。**我们手上有大量的关于日常事务的数据，当前的技术很容易处理这些数据；而对于罕见的事件，我们得到的数据非常少，且目前的技术很难去处理这些数据；
- **我们人类拥有大量的不完全信息推理的技巧，也许可以克服生活中的长尾问题**。但对于目前流行的、更多依赖大数据而非推理的人工智能技术来说，长尾问题是一个非常严重的问题；
- 世上并不只有一种思维方式，因为思维并不是一个整体。相反，**思维是可以分为部分的，而且其不同部分以不同的方式运作**。例如，深度学习在识别物体方面做得相当不错，但在计划、阅读或语言理解方面做得差一些；
- **使用深度学习进行调试非常困难，因为没有人真正理解它是如何工作的，也没有人知道如何修复问题，**大众所知道的那种调试在经典编程环境中并不适用；







