### ✅构建Toy模型

```python
# Load and normalize CIFAR10
import torch
import torchvision
import torchvision.transforms as transforms

transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

batch_size = 4

trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                        download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,
                                          shuffle=True, num_workers=2)

testset = torchvision.datasets.CIFAR10(root='./data', train=False,
                                       download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,
                                         shuffle=False, num_workers=2)

classes = ('plane', 'car', 'bird', 'cat',
           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')


# Net
import torch.nn as nn
import torch.nn.functional as F


class Net(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = torch.flatten(x, 1) # flatten all dimensions except batch
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x


net = Net()


# Define a Loss function and optimizer
import torch.optim as optim

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)


# Train the network
for epoch in range(2):  # loop over the dataset multiple times

    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):
        # get the inputs; data is a list of [inputs, labels]
        inputs, labels = data

        # zero the parameter gradients
        optimizer.zero_grad()

        # forward + backward + optimize
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # print statistics
        running_loss += loss.item()
        if i % 2000 == 1999:    # print every 2000 mini-batches
            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')
            running_loss = 0.0

print('Finished Training')
```





------



### P & R & Acc & F1

```python

import numpy as np


def p_r_f1(data, thd):

    # 将>= thd 设置为1，其他设置为0
    y_pred = data[:, 1]
    y_pred[np.where(y_pred >= thd)] = 1
    y_pred[np.where(y_pred < thd)] = 0

    y_true = data[:, 2]

    tp = np.sum(np.logical_and(np.equal(1, y_true), np.equal(1, y_pred)))
    fp = np.sum(np.logical_and(np.equal(0, y_true), np.equal(1, y_pred)))
    fn = np.sum(np.logical_and(np.equal(1, y_true), np.equal(0, y_pred)))
    tn = np.sum(np.logical_and(np.equal(0, y_true), np.equal(0, y_pred)))

    precision = tp / (tp + fp)
    recall = tp / (tp + fn)
    accuracy = (tp + tn) / (tp + fp + fn + tn)
    f1_score = (2 * precision * recall) / (precision + recall)
    print("precision", precision)
    print("recall", recall)
    print("accuracy", accuracy)
    print("f1_score", f1_score)


data = [[0, 0.76, 1], [1, 0.3, 0], [2, 0.5, 0], [3, 0.76, 1]]
data = np.array(data)
p_r_f1(data, 0.5)
```



**Acc的局限：**

准确率是样本分类问题中最简单也是最直观的评价指标，但存**在明显的缺陷：**比如负样本占99%时，分类器把所有样本都预测为负样本也可以获得99%的准确率。所以，**当不同类别的样本比例非常不均衡时,占比大的类别往往成为影响准确率的最主要因素，此时准确率指标并不足以说明分类器的好坏**



**目标检测中的TP, FP, FN：**

```python
def get_TP_FP_FN(dts, gts, iou_thre):
    dts.sort(reverse=True)
    TPs, FPs = [], []

    for dt in dts:
        sign = False
        others = [iou(dt, gt) for gt in gts]
        max_iou = max(others)
        max_iou_idx = argmax(others)

        if max_iou >= iou_thre:
            sign = not sign
            TPs.append(dt)
            gts.pop(max_iou)

        if not sign:
            FPs.append(dt)

    FNs = gts

    return TPs, FPs, FNs
```



------



### K-Means

- K-Means是聚类算法中的最常用的一种，算法最大的特点是**简单，好理解，运算速度快**，但是**只能应用于连续型的数据**，并且一定要在聚类前需要**手工指定要分成几类**，**采用欧式距离作为相似性的评价指标**，即认为两个对象的距离越近，其相似度就越大。该算法认为簇是由距离靠近的对象组成的，因此把得到紧凑且独立的簇作为最终目标；
- **K值怎么来进行确定？**(轮廓系数法)

```
[选择K个点作为初始质心

repeat 

    将每个点指派到最近的质心，形成K个簇 

    重新计算每个簇的质心 

until 簇不发生变化或达到最大迭代次数 ]
```



```python
import numpy as np
import random
import collections


def distance(point1, point2):  # 计算距离（欧几里得距离）
    return np.sqrt(np.sum((point1 - point2) ** 2))


def k_means(data, k, max_iter=1000):
    centers = {}  # 初始聚类中心
    # 初始化，选k个样本作为初始聚类中心
    # random.sample(): 随机不重复抽取k个值
    for idx, i in enumerate(random.sample(range(data.shape[0]), k)):
        # idx取值范围[0, k-1]，代表第几个聚类中心;  i为随机选取的样本作为聚类中心的编号
        centers[idx] = data[i]

    for i in range(max_iter):
        print("第{}次迭代".format(i + 1))
        clusters = collections.defaultdict(list)  # 聚类结果，聚类中心的 索引idx -> [样本集合]

        for row in data:  # 遍历每个样本
            distances = []  # 计算该样本到每个聚类中心的距离 (只会有k个元素)
            for c in centers:  # 遍历每个聚类中心
                distances.append(distance(row, centers[c]))
            idx = np.argmin(distances)  # 最小距离的索引
            clusters[idx].append(row)  # 将该样本添加到第idx个聚类中心

        pre_centers = centers.copy()  # 记录之前的聚类中心点

        for c in clusters.keys():
            # 重新计算中心点（计算该聚类中心的所有样本的均值）
            centers[c] = np.mean(clusters[c], axis=0)

        optimizer = True
        for c in centers:
            if distance(pre_centers[c], centers[c]) > 1e-8:  # 中心点是否变化
                optimizer = False
                break

        if optimizer == True:
            print("新旧聚类中心不变，迭代停止")
            break
    return centers


def predict(p_data, centers):
    distances = [distance(p_data, centers[c]) for c in centers]
    return np.argmin(distances)


x = np.random.randint(0, 10, size=(200, 2))
centers = k_means(x, 3)
print(centers)
p_data = [4, 7]
print("{}应该属于第{}个聚类中心".format(p_data, predict(p_data, centers)))
```



**简要阐述一下KNN算法和K-Means算法的区别**

①**KNN算法是分类算法**，分类算法肯定是需要有学习语料，然后通过学习语料的学习之后的模板来匹配我们的测试语料集，将测试语料集合进行按照预先学习的语料模板来分类；

②**Kmeans算法是聚类算法**，聚类算法与分类算法最大的区别是**聚类算法没有学习语料集合**；



------



### IoU

```python
import numpy as np


def get_max_IoU(pre_bboxs, gt_bbox):

    if pred_bboxes.shape[0] > 0:
        # -----0---- get coordinates of inters, but with multiple predict bboxes
        xmin = np.maximum(pred_bboxes[:, 0], gt_bbox[0])
        ymin = np.maximum(pred_bboxes[:, 1], gt_bbox[1])
        xmax = np.minimum(pred_bboxes[:, 2], gt_bbox[2])
        ymax = np.minimum(pred_bboxes[:, 3], gt_bbox[3])

        w = np.maximum(xmax - xmin + 1, 0)
        h = np.maximum(ymax - ymin + 1, 0)

        # -----1----- intersection
        inters = w * h

        # -----2----- union, uni = S1 + S2 - inters
        unions = (gt_bbox[2] - gt_bbox[0] + 1) * (gt_bbox[3] - gt_bbox[1] + 1) + (
                    pred_bboxes[:, 2] - pred_bboxes[:, 0] + 1) * (pred_bboxes[:, 3] - pred_bboxes[:, 1] + 1) - inters

        # -----3----- iou, get iou scores, max score and max iou index
        overlaps = inters / unions
        overlaps_max = np.max(overlaps)
        overlaps_max_idx = np.argmax(overlaps)

        return overlaps, overlaps_max, overlaps_max_idx


if __name__ == "__main__":
    gt_bbox = np.array([70, 80, 120, 150])

    pred_bboxes = np.array([[15, 18, 47, 60],    # top-left: <50, 50>, bottom-down: <90, 100>, <x-axis, y-axis>
                            [50, 50, 90, 100],
                            [70, 80, 120, 145],
                            [130, 160, 250, 280],
                            [25.6, 66.1, 113.3, 147.8]])
    overlaps, overlaps_max, overlaps_max_idx = get_max_IoU(pred_bboxes, gt_bbox)
    print("get_max_IoU:", "overlaps:",overlaps, "\noverlaps_max:", overlaps_max, "\noverlaps_max_idx:", overlaps_max_idx)
```



### GIoU & DIoU & CIoU

```python
def bbox_iou(box1, box2, x1y1x2y2=True, GIoU=False, DIoU=False, CIoU=False, eps=1e-7):
    # Returns the IoU of box1 to box2. box1 is 4, box2 is nx4
    box2 = box2.T

    # Get the coordinates of bounding boxes
    if x1y1x2y2:  # x1, y1, x2, y2 = box1
        b1_x1, b1_y1, b1_x2, b1_y2 = box1[0], box1[1], box1[2], box1[3]
        b2_x1, b2_y1, b2_x2, b2_y2 = box2[0], box2[1], box2[2], box2[3]
    else:  # transform from xywh to xyxy
        b1_x1, b1_x2 = box1[0] - box1[2] / 2, box1[0] + box1[2] / 2
        b1_y1, b1_y2 = box1[1] - box1[3] / 2, box1[1] + box1[3] / 2
        b2_x1, b2_x2 = box2[0] - box2[2] / 2, box2[0] + box2[2] / 2
        b2_y1, b2_y2 = box2[1] - box2[3] / 2, box2[1] + box2[3] / 2

    # Intersection area
    inter = (torch.min(b1_x2, b2_x2) - torch.max(b1_x1, b2_x1)).clamp(0) * \
            (torch.min(b1_y2, b2_y2) - torch.max(b1_y1, b2_y1)).clamp(0)

    # Union Area
    w1, h1 = b1_x2 - b1_x1, b1_y2 - b1_y1 + eps
    w2, h2 = b2_x2 - b2_x1, b2_y2 - b2_y1 + eps
    union = w1 * h1 + w2 * h2 - inter + eps

    iou = inter / union
    if GIoU or DIoU or CIoU:
        cw = torch.max(b1_x2, b2_x2) - torch.min(b1_x1, b2_x1)  # convex (smallest enclosing box) width
        ch = torch.max(b1_y2, b2_y2) - torch.min(b1_y1, b2_y1)  # convex height
        if CIoU or DIoU:  # Distance or Complete IoU https://arxiv.org/abs/1911.08287v1
            c2 = cw ** 2 + ch ** 2 + eps  # convex diagonal squared
            rho2 = ((b2_x1 + b2_x2 - b1_x1 - b1_x2) ** 2 +
                    (b2_y1 + b2_y2 - b1_y1 - b1_y2) ** 2) / 4  # center distance squared
            if DIoU:
                return iou - rho2 / c2  # DIoU
            elif CIoU:  # https://github.com/Zzh-tju/DIoU-SSD-pytorch/blob/master/utils/box/box_utils.py#L47
                v = (4 / math.pi ** 2) * torch.pow(torch.atan(w2 / h2) - torch.atan(w1 / h1), 2)
                with torch.no_grad():
                    alpha = v / (v - iou + (1 + eps))
                return iou - (rho2 / c2 + v * alpha)  # CIoU
        else:  # GIoU https://arxiv.org/pdf/1902.09630.pdf
            c_area = cw * ch + eps  # convex area
            return iou - (c_area - union) / c_area  # GIoU
    else:
        return iou  # IoU
```



------



### ROC & AUC



**ROC曲线(受试者工作特征曲线)；纵坐标：TP	横坐标：FP	ROC曲线过（0， 0）和（1， 1）**   



**截断点**是指区分正负预测结果的阈值，绘制AUC曲线需要不断移动“截断点”来得到所有的(FPR，TPR)点，然后把这些点用**线段**连起来就是ROC曲线了



AUC即ROC曲线下的面积，**AUC越大，说明分类器越可能把正样本排在前面，衡量的是一种排序的性能；**

**AUC的定义：随机从正样本和负样本中各选一个，分类器对于该正样本打分大于该负样本打分的概率**

```
如果ROC面积越大，说明曲线越往左上角靠过去。那么对于任意截断点，(FPR，TPR)坐标点越往左上角（0,1）靠，说明FPR较小趋于0（根据定义得知，就是在所有真实负样本中，基本没有预测为正的样本），TRP较大趋于1（根据定义得知，也就是在所有真实正样本中，基本全都是预测为正的样本）。并且上述是对于任意截断点来说的，很明显，那就是分类器对正样本的打分基本要大于负样本的打分（一般预测值也叫打分），衡量的就是排序能力;
```



**AUC计算**

假设数据集一共有**M个正样本，N个负样本**，预测值也就是M+N个。我**们将所有样本按照预测值进行从小到大排序，并排序编号由1到M+N**

- 对于正样本概率最大的，假设排序编号为 $\operatorname{rank}_{1}$, 比它概率小的负样本个数 $=\operatorname{ran} k_{1}-M$ ; 

- 对于正样本概率第二大的，假设排序编号为 $\operatorname{rank}_{2}$, 比它概率小的负样本个数= $\operatorname{rank}_{2}-(M-1)$
- 以此类推...... 

- 对于正样本概率最小的，假设排序编号为 $\operatorname{rank}_{M}$, 比它概率小的负样本个数 $=\operatorname{ran} k_{M}-1$
- 那么在所有情况下，正样本打分大于负样本的个数= $\operatorname{ran} k_{1}+\operatorname{ran} k_{2}+\ldots+\operatorname{rank}_{M}-(1+2+\ldots+M)_{\circ}$


$$
A U C=\frac{\sum_{i \in \text { 正样本 }} \operatorname{rank}(i)-\frac{M \times(1+M)}{2}}{M \times N}
$$

$\operatorname{rank}(i)$ 表示正样本 $i$ 的排序编号, $\quad M \times N$ 表示随机从正负样本各取一个的情况数



**对于预测值一样的样本，我们将这些样本原先的排号平均一下，作为新的排序编号就可以了**

![img](https://pic2.zhimg.com/80/v2-b2fb845ab20dcb5185a520f58f201589_1440w.jpg)

上述方法实现：

```python
import numpy as np
import pandas as pd


def AUC(y_true, y_pred):
    pair = list(zip(y_true, y_pred))
    pair = sorted(pair, key=lambda x: x[1])  # 进行排序
    df = pd.DataFrame([[x[0], x[1], i + 1] for i, x in enumerate(pair)], columns=['y_true', 'y_pred', 'rank'])

    # 下面为预测值一样的序号进行重新编号
    for k, v in df.y_pred.value_counts().items():
        if v == 1:  # 预测值k只出现了一次，continue
            continue
        rank_mean = df[df.y_pred == k]['rank'].mean()
        df.loc[df.y_pred == k, 'rank'] = rank_mean

    pos_df = df[df.y_true == 1]  # 正样本的表
    m = pos_df.shape[0]  # 正样本数
    n = df.shape[0] - m  # 负样本数
    return (pos_df['rank'].sum() - m * (m + 1) / 2) / (m * n)


y_pred = np.array(list(np.random.uniform(0.4, 0.6, 2000)) + list(np.random.uniform(0.5, 0.7, 8000)))
y_true = np.array([0] * 2000 + [1] * 8000)

print(AUC(y_true, y_pred))
```

实现二：

```python
import numpy as np

def get_AUC(y_pred, y_true):
    pos, neg = y_true[y_true == 1], y_true[y_true == 0]
    # 按概率大小逆序排列
    rank = np.argsort(y_pred)[::-1]
    y_pred, y_true = y_pred[rank], y_true[rank]
    tpr_all, fpr_all = [0], [0]
    tpr, fpr = 0, 0
    x_step, y_step = 1 / float(len(neg)), 1 / float(len(pos))
    y_sum = 0  # 用于计算AUC
    for i in range(len(y_pred)):
        if y_true[i] == 1:
            tpr += y_step
            tpr_all.append(tpr)
            fpr_all.append(fpr)
        else:
            fpr += x_step
            fpr_all.append(fpr)
            tpr_all.append(tpr)
            y_sum += tpr

    return tpr_all, fpr_all, y_sum * x_step  # 获得总体TPR，FPR和相应的AUC


y_pred = np.array(list(np.random.uniform(0.4, 0.6, 2000)) + list(np.random.uniform(0.5, 0.7, 8000)))
y_true = np.array([0] * 2000 + [1] * 8000)
tpr, fpr, auc = get_AUC(y_pred, y_true)
print(auc)
```



**AUC优点**

- **AUC衡量的是一种排序能力**，因此特别适合排序类业务；
- **AUC对正负样本均衡并不敏感**，在样本不均衡的情况下，也可以做出合理的评估；
- 其他指标比如P，R，F1，根据区分正负样本阈值的变化会有不同的结果，而**AUC不需要手动设定阈值，是一种整体上的衡量方法**；

**AUC缺点**

- **忽略了预测的概率值和模型的拟合程度；**
- **没有给出模型误差的空间分布信息**：AUC只关注正负样本之间的排序，并不关心正样本内部或者负样本内部的排序，这样我们也无法衡量样本对于客户的好坏程度的刻画能力；
- **AUC反应了太过于整体的信息**，无法反应P，R，F1等在实际业务中经常关心的指标；



https://zhuanlan.zhihu.com/p/360765777

排序复杂度：O(log2(P+N))   计算AUC的复杂度：O(P+N)

放缩结果对AUC没有影响



------



### AP

```
mAP: 各类别AP的平均值

AP: PR曲线下面积

PR曲线: Precision-Recall曲线

Precision: TP / (TP + FP)

Recall: TP / (TP + FN)


TP: IoU>0.5的检测框数量（同一Ground Truth只计算一次）

FP: IoU<=0.5的检测框，或者是检测到同一个GT的多余检测框的数量

FN: 没有检测到的GT的数量
```



```
mAP计算：在VOC2010及以后，需要针对每一个不同的Recall值（包括0和1），选取其大于等于这些Recall值时的Precision最大值，然后计算PR曲线下面积作为AP值
```



------



### 什么时候用 ROC & Precision-Recall

- **Data Imbalanced** 的情况用 **Precision-Recall**；
- 如果 Positive，Negative 的数量基本是平衡的，那 **ROC**就更常用一些；



### NMS



**算法输入：**算法对一幅图产生的所有的候选框，每个框有坐标与对应的置信度；

**算法输出：**输入的一个子集，同样是一组5维数组，表示筛选后的边界框；

**算法流程：**

1. 将所有的框按类别划分，并**剔除背景类**，因为无需NMS；
2. 对每个物体类中的边界框(B_BOX)，按照分类**置信度降序**排列；
3. 在某一类中，选择置信度最高的边界框B_BOX1，将B_BOX1从输入列表中去除，并加入输出列表；
4. 逐个计算B_BOX1与其余B_BOX2的交并比IoU，若IoU(B_BOX1,B_BOX2) > 阈值TH，则在输入去除B_BOX2；
5. 重复步骤3~4，直到输入列表为空，完成一个物体类的遍历；
6. 重复2~5，直到所有物体类的NMS处理完成；
7. 输出列表，算法结束；

```python
import numpy as np


def nms(dets, thresh):
    # dets某个类的框，x1、y1、x2、y2、以及置信度score
    # eg:dets为[[x1,y1,x2,y2,score],[x1,y1,y2,score]……]]
    # thresh是IoU的阈值
    x1, y1 = dets[:, 0], dets[:, 1]
    x2, y2 = dets[:, 2], dets[:, 3]
    scores = dets[:, 4]
    # 每一个检测框的面积
    areas = (x2 - x1 + 1) * (y2 - y1 + 1)
    # 按照score置信度降序排序
    order = np.argsort(scores)[::-1]
    keep = []  # 保留的结果框集合
    while order.size > 0:
        i = order[0]  # i表示box得分最高的索引
        keep.append(i)  # 保留该类剩余box中得分最高的一个
        # 得到相交区域,左上及右下
        xmin = np.maximum(x1[i], x1[order[1:]])
        ymin = np.maximum(y1[i], y1[order[1:]])
        xmax = np.minimum(x2[i], x2[order[1:]])
        ymax = np.minimum(y2[i], y2[order[1:]])
        # 计算相交的面积,不重叠时面积为0
        w = np.maximum(0, xmax - xmin + 1)
        h = np.maximum(0, ymax - ymin + 1)
        inter = w * h
        # 计算IoU：重叠面积 /（面积1+面积2-重叠面积）
        ovr = inter / (areas[i] + areas[order[1:]] - inter)
        # 保留IoU小于阈值的box
        inds = np.where(ovr <= thresh)[0]  # 返回坐标,over和areas[order[1:]]一样是n-1 x 1
        order = order[inds + 1]  # 因为ovr数组的长度比order数组少一个（0是选取的基准）,所以这里要将所有下标后移一位
    return keep


dets = np.array([
    [11.5, 12, 311.4, 410.6, 0.85],
    [0.5, 1, 300.4, 400.5, 0.97],
    [200.5, 300, 700.4, 1000.6, 0.65],
    [250.5, 310, 700.4, 1000.6, 0.72]
])
res = nms(dets, 0.5)
print(dets[res])
```



------



### ✅Torchvision.ops.Focal_Loss

- **alpha:** (optional) Weighting factor in range (0,1) to balance **positive vs negative examples** or -1 for ignore. Default = 0.25；
- **gamma:** Exponent of the modulating factor (1 - p_t) to balance **easy vs hard examples；**

```python
import torch
import torch.nn.functional as F

from ..utils import _log_api_usage_once



[docs]def sigmoid_focal_loss(
    inputs: torch.Tensor,
    targets: torch.Tensor,
    alpha: float = 0.25,
    gamma: float = 2,
    reduction: str = "none",
):
    """
    Original implementation from https://github.com/facebookresearch/fvcore/blob/master/fvcore/nn/focal_loss.py .
    Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.

    Args:
        inputs: A float tensor of arbitrary shape.
                The predictions for each example.
        targets: A float tensor with the same shape as inputs. Stores the binary
                classification label for each element in inputs
                (0 for the negative class and 1 for the positive class).
        alpha: (optional) Weighting factor in range (0,1) to balance
                positive vs negative examples or -1 for ignore. Default = 0.25
        gamma: Exponent of the modulating factor (1 - p_t) to
               balance easy vs hard examples.
        reduction: 'none' | 'mean' | 'sum'
                 'none': No reduction will be applied to the output.
                 'mean': The output will be averaged.
                 'sum': The output will be summed.
    Returns:
        Loss tensor with the reduction option applied.
    """
    if not torch.jit.is_scripting() and not torch.jit.is_tracing():
        _log_api_usage_once(sigmoid_focal_loss)
        
    p = torch.sigmoid(inputs)
    ce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction="none")
    p_t = p * targets + (1 - p) * (1 - targets)
    loss = ce_loss * ((1 - p_t) ** gamma)

    if alpha >= 0:
        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)
        loss = alpha_t * loss

    if reduction == "mean":
        loss = loss.mean()
    elif reduction == "sum":
        loss = loss.sum()

    return loss
```



------



### Dropout

```python
 def dropout(x, level):  # level为神经元丢弃的概率值，在0-1之间
    if level < 0 or level >= 1:
        raise Exception('Dropout level must be in interval [0, 1[.')  
    retain_prob = 1. - level  
    # 利用binomial函数，生成与x一样的维数向量。
    # 神经元x保留的概率为p，n表示每个神经元参与随机实验的次数，通常为1,。
    # size是神经元总数。  
    sample = np.random.binomial(n=1, p=retain_prob, size=x.shape)
    # 生成一个0、1分布的向量，0表示该神经元被丢弃 
    # print sample  
    x *=sample
    # print x  
    x /= retain_prob  
    return x 
```



### OneCycleLR

```python
def one_cycle(y1=0.0, y2=1.0, steps=100):
    # lambda function for sinusoidal ramp from y1 to y2
    return lambda x: ((1 - math.cos(x * math.pi / steps)) / 2) * (y2 - y1) + y1
```

**Cyclic LR**:每隔一段时间重启学习率，这样在单位时间内能收敛到多个局部最小值，可以得到很多个模型做集成



### ViT

![image](https://user-images.githubusercontent.com/16083536/109922887-0525db00-7cf9-11eb-9986-b9df962628e3.png)

```python
class TransformerLayer(nn.Module):
    # Transformer layer https://arxiv.org/abs/2010.11929 (LayerNorm layers removed for better performance)
    def __init__(self, c, num_heads):
        super().__init__()
        self.q = nn.Linear(c, c, bias=False)
        self.k = nn.Linear(c, c, bias=False)
        self.v = nn.Linear(c, c, bias=False)
        self.ma = nn.MultiheadAttention(embed_dim=c, num_heads=num_heads)
        self.fc1 = nn.Linear(c, c, bias=False)
        self.fc2 = nn.Linear(c, c, bias=False)

    def forward(self, x):
        x = self.ma(self.q(x), self.k(x), self.v(x))[0] + x
        x = self.fc2(self.fc1(x)) + x
        return x


class TransformerBlock(nn.Module):
    # Vision Transformer https://arxiv.org/abs/2010.11929
    def __init__(self, c1, c2, num_heads, num_layers):
        super().__init__()
        self.conv = None
        if c1 != c2:
            self.conv = Conv(c1, c2)
        self.linear = nn.Linear(c2, c2)  # learnable position embedding
        self.tr = nn.Sequential(*[TransformerLayer(c2, num_heads) for _ in range(num_layers)])
        self.c2 = c2

    def forward(self, x):
        if self.conv is not None:
            x = self.conv(x)
        b, _, w, h = x.shape
        p = x.flatten(2)
        p = p.unsqueeze(0)
        p = p.transpose(0, 3)
        p = p.squeeze(3)
        e = self.linear(p)
        x = p + e

        x = self.tr(x)
        x = x.unsqueeze(3)
        x = x.transpose(0, 3)
        x = x.reshape(b, self.c2, w, h)
        return x
   

class C3TR(C3):
    # C3 module with TransformerBlock()
    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        c_ = int(c2 * e)
        self.m = TransformerBlock(c_, c_, 4, n)
```



### Mosaic

```python
def load_mosaic(self, index):
    # loads images in a 4-mosaic

    labels4, segments4 = [], []
    s = self.img_size
    # 随机取mosaic中心点
    # self.mosaic_border = [-img_size // 2, -img_size // 2]
    yc, xc = [int(random.uniform(-x, 2 * s + x)) for x in self.mosaic_border]  # mosaic center x, y  
    # 随机取其他三张图片的索引
    indices = [index] + random.choices(self.indices, k=3)  # 3 additional image indices
    for i, index in enumerate(indices):
        # Load image
        img, _, (h, w) = load_image(self, index)

        # place img in img4
        if i == 0:  # top left
            # 初始化大图img4,114灰度填充
            img4 = np.full((s * 2, s * 2, img.shape[2]), 114, dtype=np.uint8)  # base image with 4 tiles
            # 设置大图上的位置（左上角）
            x1a, y1a, x2a, y2a = max(xc - w, 0), max(yc - h, 0), xc, yc  # xmin, ymin, xmax, ymax (large image)
            # 选取小图img上的位置
            x1b, y1b, x2b, y2b = w - (x2a - x1a), h - (y2a - y1a), w, h  # xmin, ymin, xmax, ymax (small image)
        elif i == 1:  # top right
            x1a, y1a, x2a, y2a = xc, max(yc - h, 0), min(xc + w, s * 2), yc
            x1b, y1b, x2b, y2b = 0, h - (y2a - y1a), min(w, x2a - x1a), h
        elif i == 2:  # bottom left
            x1a, y1a, x2a, y2a = max(xc - w, 0), yc, xc, min(s * 2, yc + h)
            x1b, y1b, x2b, y2b = w - (x2a - x1a), 0, w, min(y2a - y1a, h)
        elif i == 3:  # bottom right
            x1a, y1a, x2a, y2a = xc, yc, min(xc + w, s * 2), min(s * 2, yc + h)
            x1b, y1b, x2b, y2b = 0, 0, min(w, x2a - x1a), min(y2a - y1a, h)
				# 将小图上截取的部分贴到大图上
        img4[y1a:y2a, x1a:x2a] = img[y1b:y2b, x1b:x2b]  # img4[ymin:ymax, xmin:xmax]
        # 计算小图到大图上时所产生的偏移，用来计算mosaic增强后的标签框的位置
        padw = x1a - x1b
        padh = y1a - y1b

        # Labels
        labels, segments = self.labels[index].copy(), self.segments[index].copy()
        # 重新调整标签框的位置
        if labels.size:
            labels[:, 1:] = xywhn2xyxy(labels[:, 1:], w, h, padw, padh)  # normalized xywh to pixel xyxy format
            segments = [xyn2xy(x, w, h, padw, padh) for x in segments]
        labels4.append(labels)
        segments4.extend(segments)

    # Concat/clip labels
    # 调整标签框在图片内部
    labels4 = np.concatenate(labels4, 0)
    for x in (labels4[:, 1:], *segments4):
        np.clip(x, 0, 2 * s, out=x)  # clip when using random_perspective()
    # img4, labels4 = replicate(img4, labels4)  # replicate
		
    # 进行mosaic的时候将四张图片整合到一起之后shape为[2*img_size, 2*img_size]
    # 对mosaic整合的图片进行随机旋转、平移、缩放、裁剪，并resize为输入大小img_size
    # Augment
    img4, labels4 = random_perspective(img4, labels4, segments4,
                                       degrees=self.hyp['degrees'],
                                       translate=self.hyp['translate'],
                                       scale=self.hyp['scale'],
                                       shear=self.hyp['shear'],
                                       perspective=self.hyp['perspective'],
                                       border=self.mosaic_border)  # border to remove

    return img4, labels4
```

