

## 基础Q&A



#### Anchor-Base缺点

- 检测性能**对于anchor的大小，数量，长宽比都非常敏感**，这些固定的anchor极大地**损害了检测器的泛化性**，导致对于不同任务，其anchor都必须重新设置大小和长宽比；
- 为了去匹配真实框，需要生成大量的anchor，但是大部分的anchor在训练时标记为negative，所以就造成**正负样本的不平衡**；
- 在训练中，需要**计算所有anchor与真实框的IOU**，这样就会**消耗大量内存和时间**；



#### Anchor-Free缺点

- **语义模糊性**，即两个物体的中心点落在了同一个网格中 ：
  - FCOS默认将该点分配给面积最小的目标；
  - 使用FPN界定每个特征层的检测范围；
  - center sampling准则；【只有GT bbox中心附近的一定范围内的小bbox内的点，分类时才作为正样本】
- anchor free缺少先验知识，所以优化不如anchor based的方法**稳定**；



#### YOLOv5和YOLOX如何选择

- 数据及分辨率<= 640使用YOLOX，>=1280使用YOLOv5（更大预训练weights）；



## 模型讲解



### Anchor-Based

#### SSD



#### RetinaNet



#### Faster RCNN



#### YOLOv1



#### YOLOv3



#### YOLOv5 



![yolov5](%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B.assets/172404576-c260dcf9-76bb-4bc8-b6a9-f2d987792583.png)



**网络结构：**

- **Focus-> 6x6conv**【类似于Swin Transformer的Patch Merging，没有特征损失】；

<img src="%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B.assets/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5aSq6Ziz6Iqx55qE5bCP57u_6LGG,size_20,color_FFFFFF,t_70,g_se,x_16.png" alt="在这里插入图片描述" style="zoom: 67%;" />



- **SPP -> SPPF**【串联多个Maxpooling，两者等效，但更效率】，串行两个`5x5`大小的`MaxPool`层是和一个`9x9`大小的`MaxPool`层计算结果是一样的，串行三个`5x5`大小的`MaxPool`层是和一个`13x13`大小的`MaxPool`层计算结果是一样的；

<img src="%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B.assets/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5aSq6Ziz6Iqx55qE5bCP57u_6LGG,size_20,color_FFFFFF,t_70,g_se,x_16-20220611195625836.png" alt="在这里插入图片描述" style="zoom:67%;" />



**数据增强：**

- Mosaic、Copy paste【数据集要有`segments`数据】、仿射变换、MixUp、随机调整HSV、随机水平翻转；



**训练策略：**

- 多尺度训练【0.5 - 1.5x】、AutoAnchor、EMA、冻结训练、Warmup and Cosine LR scheduler、混合精度、超参数优化



**损失计算：**

- **Classes loss**，采用的是`BCE loss`，只计算正样本；
- **Objectness loss**，采用的依然是`BCE loss`，指的是网络预测的目标边界框与GT Box的`CIoU`，计算所有样本；
- **Location loss**，采用的是`CIoU loss`，注意只计算正样本；

$$
\text { Loss }=\lambda_{1} L_{c l s}+\lambda_{2} L_{o b j}+\lambda_{3} L_{l o c}
$$

**平衡不同尺度损失：**
$$
L_{o b j}=4.0 \cdot L_{o b j}^{\text {small }}+1.0 \cdot L_{o b j}^{\text {medium }}+0.4 \cdot L_{o b j}^{\text {large }}
$$
针对预测小目标的预测特征层（`P3`）采用的权重是`4.0`，针对预测中等目标的预测特征层（`P4`）采用的权重是`1.0`，针对预测大目标的预测特征层（`P5`）采用的权重是`0.4`；



**消除Grid敏感度：**

<img src="%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B.assets/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5aSq6Ziz6Iqx55qE5bCP57u_6LGG,size_11,color_FFFFFF,t_70,g_se,x_16.png" alt="在这里插入图片描述" style="zoom:67%;" />



- YOLOv4以前使用上图计算方法，但是比如**当真实目标中心点非常靠近网格的左上角点或者右下角点时**，网络的预测值需要负无穷或者正无穷时才能取到，而这种很极端的值网络一般无法达到【sigmoid需要取值-inf或者inf】。为了解决这个问题，YOLOv4对偏移量进行了缩放从原来的( 0 , 1 ) 缩放到( − 0.5 , 1.5 ) ，这样网络预测的偏移量就能很方便达到0或1，故最终预测的目标中心点b x , b y  的计算公式为：
  $$
  \begin{aligned}
  &b_{x}=\left(2 \cdot \sigma\left(t_{x}\right)-0.5\right)+c_{x} \\
  &b_{y}=\left(2 \cdot \sigma\left(t_{y}\right)-0.5\right)+c_{y}
  \end{aligned}
  $$

- YOLOv5调整了预测目标高宽的计算公式：【原来的计算公式并没有对预测目标宽高做限制，这样可能出现梯度爆炸，训练不稳定等问题，调整后倍率因子被限制在( 0 , 4 )之间】

$$
\begin{aligned}
b_{w} &=p_{w} \cdot\left(2 \cdot \sigma\left(t_{w}\right)\right)^{2} \\
b_{h} &=p_{h} \cdot\left(2 \cdot \sigma\left(t_{h}\right)\right)^{2}
\end{aligned}
$$



**正样本匹配：**

- YOLOv4和YOLOv5主要的区别在于`GT Box`与`Anchor Templates`模板的匹配方式；

- YOLOv4中是直接将每个`GT Box`与对应的`Anchor Templates`模板计算`IoU`，只要`IoU`大于设定的阈值就算匹配成功；

  

1.YOLOv5先去计算每个`GT Box`与对应的`Anchor Templates`模板的高宽比例：
$$
\begin{aligned}
r_{w} &=w_{g t} / w_{a t} \\
r_{h} &=h_{g t} / h_{a t}
\end{aligned}
$$
2.然后统计这些比例和它们倒数之间的最大值，这里可以理解成计算`GT Box`和`Anchor Templates`分别在宽度以及高度方向的最大差异（当相等的时候比例为1，差异最小）：
$$
\begin{aligned}
r_{w}^{\max } &=\max \left(r_{w}, 1 / r_{w}\right) \\
r_{h}^{\max } &=\max \left(r_{h}, 1 / r_{h}\right)
\end{aligned}
$$
3.接着统计r_max_w和r_max_h之间的最大值（宽度和高度方向差异最大的值）：
$$
r^{\max }=\max \left(r_{w}^{\max }, r_{h}^{\max }\right)
$$
4.如果GT Box和对应的Anchor Template的r_max小于阈值anchor_t（在源码中默认设置为4.0），即GT Box和对应的Anchor Template的高、宽比例相差不算太大，则将GT Box分配给该Anchor Template，示意图如下：

<img src="%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B.assets/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5aSq6Ziz6Iqx55qE5bCP57u_6LGG,size_20,color_FFFFFF,t_70,g_se,x_16-20220611202519672.png" alt="在这里插入图片描述" style="zoom: 50%;" />



5.将`GT`投影到对应预测特征层上，根据`GT`的中心点定位到对应`Cell`，因为网络预测中心点的偏移范围已经调整到了( − 0.5 , 1.5 ) ，所以按理说只要Grid Cell左上角点距离GT中心点在( − 0.5 , 1.5 ) 范围内它们对应的Anchor都能回归到GT的位置处，这样会让正样本的数量得到大量的扩充，YOLOv5源码中扩展`Cell`时只会往上、下、左、右四个方向扩展，下图中其中`%1`表示取余并保留小数部分；

![在这里插入图片描述](%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B.assets/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5aSq6Ziz6Iqx55qE5bCP57u_6LGG,size_20,color_FFFFFF,t_70,g_se,x_16-20220611204501721.png)



#### YOLO-X



------



### Anchor-Free



#### FCOS

**跳出Anchor的限制，使用语义分割的思想逐像素来解决检测问题：**在预测特征图的每个位置上直接去预测该点分别距离目标左侧（l: left），上侧（t：top），右侧(r: right)以及下侧（b：bottom）的距离，如下图所示：

<img src="%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B.assets/%E6%88%AA%E5%B1%8F2022-06-11%2010.37.43.png" alt="截屏2022-06-11 10.37.43" style="zoom:67%;" />



**网络结构：**

<img src="%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B.assets/%E6%88%AA%E5%B1%8F2022-06-11%2010.42.48.png" alt="截屏2022-06-11 10.42.48" style="zoom:67%;" />

![图片](%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B.assets/640.png)



**输出：**

- **Classification**：在预测特征图的每个位置上都会预测C个score参数；【类别】
- **Regression**：在预测特征图的每个位置上都会预测4个距离参数l，r，t，b【预测的数值是相对特征图尺度】，假设对于预测特征图上某个点映射回原图的坐标是(C_x, C_y)，特征图相对原图的步距是`s`，那么网络预测该点对应的目标边界框坐标为：

$$
\begin{aligned}
x_{\min }=c_{x}-l \cdot s, y_{\min } &=c_{y}-t \cdot s \\
x_{\max } &=c_{x}+r \cdot s, y_{\max }=c_{y}+b \cdot s
\end{aligned}
$$

- **Center-ness：**预测特征图的每个位置上都会预测1个参数，`center-ness`反映的是该点（特征图上的某一点）距离目标中心的远近程度，它的值域在0~1之间，距离目标中心越近`center-ness`越接近于1，下面是`center-ness`真实标签的计算公式（计算损失时只考虑正样本，即预测点在目标内的情况）

$$
\text { centerness }^{*}=\sqrt{\frac{\min \left(l^{*}, r^{*}\right)}{\max \left(l^{*}, r^{*}\right)} \times \frac{\min \left(t^{*}, b^{*}\right)}{\max \left(t^{*}, b^{*}\right)}}
$$

​    在网络后处理部分筛选高质量bbox时，会将预测的目标`class score`与`center-ness`相乘再开根，然后根据得到的结果对bbox进行排序，只保留分数较高的bbox，这样做的目的是筛掉那些目标`class score`低且预测点距离目标中心较远的bbox，最终保留下来的就是高质量的bbox；



**正负样本的匹配：**

- 对于特征图上的某一点(x, y)，只要它落入GT box中心区域，那么它就被视为正样本；
- (cx , cy ) 在 (cx − rs, cy − rs, cx + rs, cy + rs)这个sub-box范围之内，其中(cx , cy ) 是GT的中心点，`s`是特征图相对原图的步距，`r`是一个超参数控制距离GT中心的远近；
- 换句话说点(x , y ) 不仅要在GT的范围内，还要离GT的中心点(cx , cy ) 足够近才能被视为正样本；



假设上面两个feature map对应的是同一个特征图，将特征图上的每个点映射回原图就是下面图片中黑色的圆点。根据2019年发表论文的匹配准则，只要落入GT box就算正样本，所以左侧的feature map中打勾的位置都被视为正样本。根据2020年的版本，不仅要落入GT Box还要在(cx − rs, cy − rs, cx + rs, cy + rs)这个`sub-box`范围内，所以右侧的feature map中打勾的位置都被视为正样本；



<img src="%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B.assets/640-20220611113248420.png" alt="图片" style="zoom: 67%;" />



**损失计算：**

损失由分类损失L_cls、定位损失L_reg以及`center-ness`损失L_ctrness三部分共同组成：

- 分类损失L_cls采用`bce_focal_loss`，即二值交叉熵损失配合`focal_loss`，计算损失时所有样本都会参与计算（正样本和负样本）;
- 定位损失L_reg采用`giou_loss`,计算损失时只有正样本参与计算;
- `center-ness`损失L_ctrness采用二值交叉熵损失，计算损失时只有正样本参与计算;


$$
\begin{aligned}
L\left(\left\{p_{x, y}\right\},\left\{t_{x, y}\right\},\left\{s_{x, y}\right\}\right) &=\frac{1}{N_{p o s}} \sum_{x, y} L_{c l s}\left(p_{x, y}, c_{x, y}^{*}\right) \\
&+\frac{1}{N_{p o s}} \sum_{x, y} 1_{\left\{c_{x, y}^{*}>0\right\}} L_{r e g}\left(t_{x, y}, t_{x, y}^{*}\right) \\
&+\frac{1}{N_{p o s}} \sum_{x, y} 1_{\left\{c_{x, y}^{*}>0\right\}} L_{c t r n e s s}\left(s_{x, y}, s_{x, y}^{*}\right)
\end{aligned}
$$

- p_(x,y)表示在特征图点(x,y)处预测的每个类别的score,c*_(x,y)表示在特征图点(x,y)对应的真实类别标签;
- l{c*_(x,y)>0}当特征图(x,y)点被匹配为正样本时为1，否则为0;
- t(x,y)表示在特征图点(x,y)处预测的目标边界框信息,t*_(x,y)表示在特征图点(x,y)对应的真实目标边界框信息;
- S(x,y)表示在特征图点(x,y)处预测的`center-ness`,S(x,y)表示在特征图点(x,y)对应的真实`center-ness`;



假设对于特征图上的某一个点（图中用蓝色填充的`cell`）映射回原图，对应图片中的黑色点。然后计算该点距离GT box左侧，上侧，右侧，下侧的距离就能得到`l*，r*，t*，b*`，再套用上面的公式就能得到s*(x,y)；

![图片](%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B.assets/640-20220611115914689.png)



**Ambiguity问题：（如果feature map上的某个点同时落入两个GT Box相交区域）**

- 默认将该点分配给**面积Area最小的GT Box**【球拍】；
- FPN：FPN中会采用多个预测特征图，不同尺度的特征图负责预测不同尺度的目标：比如P3负责预测小型目标，P5负责预测中等目标，P7负责预测大型目标，这样在匹配正负样本时能够将部分重叠在一起的目标（这里主要指不同尺度的目标）给分开；
- **center sampling匹配准则**：匹配正样本时要求不仅要落入GT Box还要在 (cx − rs, cy − rs, cx + rs, cy + rs)这个`sub-box`范围内；



**FPN每个特征层范围正负样本划分：**More specifically, we first compute the regression targets l ∗ , t ∗ , r ∗ and b ∗ for each location on all feature levels. Next, if a location at feature level i satisfies max(l ∗ , t ∗ , r∗ , b∗ ) ≤ mi−1 or max(l ∗ , t∗ , r∗ , b∗ ) ≥ mi , it is set as a negative sample and thus not required to regress a bounding box anymore. Here mi is the maximum distance that feature level i needs to regress. In this work, m2, m3, m4, m5, m6 and m7 are set as 0, 64, 128, 256, 512 and ∞, respectively.



![图片](%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B.assets/640-20220611120136742.png)



**改进**

```
bbox loss weight：所有正样本点的 weight 平权 -> 将样本点对应的 centerness 作为权重，离 GT 中心越近，权重越大
centerness 分支利用 l，t，r，b 计算 centerness -> 直接用 IoU
```



#### CornerNet



#### CenterNet



------



### Transformer-Style



#### DETR
