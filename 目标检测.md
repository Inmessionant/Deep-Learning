

## 基础 Q & A



### Anchor-Base缺点

- 检测性能**对于anchor的大小，数量，长宽比都非常敏感**，这些固定的anchor极大地**损害了检测器的泛化性**，导致对于不同任务，其anchor都必须重新设置大小和长宽比；
- 为了去匹配真实框，需要生成大量的anchor，但是大部分的anchor在训练时标记为negative，所以就造成**正负样本的不平衡**；
- 在训练中，需要**计算所有anchor与真实框的IOU**，这样就会**消耗大量内存和时间**；



------

### Anchor-Free缺点

- **语义模糊性**，即两个物体的中心点落在了同一个网格中 ：
  - FCOS默认将该点分配给面积最小的目标；
  - 使用FPN界定每个特征层的检测范围；
  - center sampling准则；【只有GT bbox中心附近的一定范围内的小bbox内的点，分类时才作为正样本】
- anchor free缺少先验知识，所以优化不如anchor based的方法**稳定**；



------

### Label Assignment

- RetinaNet根据**Anchor和目标的IoU**来确定正负样本；
- FCOS根据**目标中心区域和目标的尺度**确定正负样本；



Assign算法的原则：

- 中心先验：FCOS / CenterNet
- Loss aware（动态匹配）：FreeAnchor / ATSS
- 不同目标设定不同数量正样本（进一步动态）：PAA / AutoAssign
- 全局信息：IQDet / OTA



------

### 传统目标检测



**区域选择->特征提取->分类器**

- 使用不同尺度的滑动窗口选定图像的某一区域为候选区域；
- 从对应的候选区域提取如Harrs HOG等一类或者多类**特征**；
- 使用 SVM 等分类算法对对应的候选区域进行分类，判断是否属于待检测的目标；



**缺点**

- 基于滑动窗口的区域选择策略没有针对性，**时间复杂度高，窗口冗余；**
- 手工设计的特征对于多样性的变化没有很好的**鲁棒性**；

 

------

### 网络的分类

- [x] **基于阶段：**

  - **多阶：**Casade RCNN

   - **两阶：**RCNN / Fast RCNN / Faster RCNN

   - **单阶：**SSD / YOLO v1~v5 / RetinaNet / EfficientNet / CornerNet / FCOS

- [x] **是否使用Anchor：**

  - **Anchor Free:**
    - **Dense Prediction：**DenseBox
    - **Keypoint-based：**CenterNet / CornerNet


  - **Anchor based：**

    - **Dimension Clusters：**YOLO v2 ~ YOLO v5 / PP-YOLO / EfficientNet 

    - **Hand pickeed：**SSD / Faster RCNN


- [x] **不同标签方案：**

  - **Region proposal-based：**RCNN / Fast RCNN / Faster RCNN
  - **基于keypoint-based：**CornerNet / CenterNet / RepPoints

  - **基于author-IoU：**SSD / Faster RCNN / YOLO v2 ~ v5 / EfficientNet 



------

### 类别不平衡



**数据**

- OHEM（正负样例不平衡）
- **过采样** / 欠采样
- **增加数据**
- Input Mixup + 基于Re-Sampling的Re-Balancing + Fine-Tuning

**Loss**

- **Focal Loss**（正负样例不平衡alpha，简单困难样例不平衡belta）
- 目前普遍存在一个误解，认为focal loss是解决样本不均衡的杀器，实际上更重要的是分类层bias的初始化(yolox和v5都用了），另外在300Epoch的训练轮数下，不均衡问题也基本不是问题了

**网络**

- **BBN**



------

### ROI Pooling & Align

 

**两次整数化（量化）过程**：

- **region proposal**的xywh通常是小数，但是为了方便操作会把它整数化；
- 将整数化后的边界区域**平均分割成 k x k 个单元**，对每一个单元边界进行整数化；

**经过上述两次整数化，此时的候选框已经和最开始回归出来的位置有一定的偏差，这个偏差会影响检测或者分割的准确度；**  - >  **mis-alignment**



**ROI Align**: **取消量化操作**：

- 遍历每一个候选区域，保持浮点数边界不做量化，使用双线性内插的方法获得坐标为浮点数的像素点上的图像数值,从而**将整个特征聚集过程转化为一个连续的操作**；
- 将候选区域分割成k x k个单元，每个单元的边界也不做量化，在每个单元中计算固定四个坐标位置，用双线性内插的方法计算出这四个位置的值，然后进行最大池化操作；



------

### YOLOv5和YOLOX如何选择

- 数据及分辨率<= 640使用YOLOX，>=1280使用YOLOv5（更大预训练weights）；



## #####################################################

## Anchor-Based



### RetinaNet



### Faster RCNN



### YOLOv1



### YOLOv3



### ✅YOLOv5 



![在这里插入图片描述](https://img-blog.csdnimg.cn/15551d183d1e4a71a7456ef7ce534779.png#pic_center)



**网络结构：**

- **Focus-> 6x6conv**【类似于Swin Transformer的Patch Merging，没有特征损失】；

<img src="https://img-blog.csdnimg.cn/b8de763fa60646ebb108a45bd3fc9af4.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5aSq6Ziz6Iqx55qE5bCP57u_6LGG,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述" style="zoom:67%;" />



- **SPP -> SPPF**【串联多个Maxpooling，两者等效，但更效率】，串行两个`5x5`大小的`MaxPool`层是和一个`9x9`大小的`MaxPool`层计算结果是一样的，串行三个`5x5`大小的`MaxPool`层是和一个`13x13`大小的`MaxPool`层计算结果是一样的；

![在这里插入图片描述](https://img-blog.csdnimg.cn/aae80e4a028c47e8bf1149a333d300bd.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5aSq6Ziz6Iqx55qE5bCP57u_6LGG,size_20,color_FFFFFF,t_70,g_se,x_16)



**数据增强：**

- Mosaic、Copy paste【数据集要有`segments`数据】、仿射变换、MixUp、随机调整HSV、随机水平翻转；



**训练策略：**

- 多尺度训练【0.5 - 1.5x】、AutoAnchor、EMA、冻结训练、Warmup and Cosine LR scheduler、混合精度、超参数优化



**损失计算：**

- **Classes loss**，采用的是`BCE loss`，只计算正样本；
- **Objectness loss**，采用的依然是`BCE loss`，指的是网络预测的目标边界框与GT Box的`CIoU`，计算所有样本；
- **Location loss**，采用的是`CIoU loss`，注意只计算正样本；

$$
\text { Loss }=\lambda_{1} L_{c l s}+\lambda_{2} L_{o b j}+\lambda_{3} L_{l o c}
$$

**平衡不同尺度损失：**
$$
L_{o b j}=4.0 \cdot L_{o b j}^{\text {small }}+1.0 \cdot L_{o b j}^{\text {medium }}+0.4 \cdot L_{o b j}^{\text {large }}
$$
针对预测小目标的预测特征层（`P3`）采用的权重是`4.0`，针对预测中等目标的预测特征层（`P4`）采用的权重是`1.0`，针对预测大目标的预测特征层（`P5`）采用的权重是`0.4`；



**消除Grid敏感度：**

![在这里插入图片描述](https://img-blog.csdnimg.cn/9dbbecc8cb0f4981b3da423740d7a8ec.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5aSq6Ziz6Iqx55qE5bCP57u_6LGG,size_11,color_FFFFFF,t_70,g_se,x_16#pic_center)



- YOLOv4以前使用上图计算方法，但是比如**当真实目标中心点非常靠近网格的左上角点或者右下角点时**，网络的预测值需要负无穷或者正无穷时才能取到，而这种很极端的值网络一般无法达到（sigmoid需要取值-inf或者inf）。为了解决这个问题，YOLOv4对偏移量进行了缩放从原来的( 0 , 1 ) 缩放到( − 0.5 , 1.5 ) ，这样网络预测的偏移量就能很方便达到0或1，故最终预测的目标中心点b x , b y  的计算公式为：
  $$
  \begin{aligned}
  &b_{x}=\left(2 \cdot \sigma\left(t_{x}\right)-0.5\right)+c_{x} \\
  &b_{y}=\left(2 \cdot \sigma\left(t_{y}\right)-0.5\right)+c_{y}
  \end{aligned}
  $$

- YOLOv5调整了预测目标高宽的计算公式：原来的计算公式并没有对预测目标宽高做限制，这样可能出现梯度爆炸，训练不稳定等问题，调整后倍率因子被限制在( 0 , 4 )之间

$$
\begin{aligned}
b_{w} &=p_{w} \cdot\left(2 \cdot \sigma\left(t_{w}\right)\right)^{2} \\
b_{h} &=p_{h} \cdot\left(2 \cdot \sigma\left(t_{h}\right)\right)^{2}
\end{aligned}
$$



**正样本匹配：**

- YOLOv4和YOLOv5主要的区别在于`GT Box`与`Anchor Templates`模板的匹配方式；

- YOLOv4中是直接将每个`GT Box`与对应的`Anchor Templates`模板计算`IoU`，只要`IoU`大于设定的阈值就算匹配成功；

  

1.YOLOv5先去计算每个`GT Box`与对应的`Anchor Templates`模板的高宽比例：
$$
\begin{aligned}
r_{w} &=w_{g t} / w_{a t} \\
r_{h} &=h_{g t} / h_{a t}
\end{aligned}
$$
2.然后统计这些比例和它们倒数之间的最大值，这里可以理解成计算`GT Box`和`Anchor Templates`分别在宽度以及高度方向的最大差异（当相等的时候比例为1，差异最小）：
$$
\begin{aligned}
r_{w}^{\max } &=\max \left(r_{w}, 1 / r_{w}\right) \\
r_{h}^{\max } &=\max \left(r_{h}, 1 / r_{h}\right)
\end{aligned}
$$
3.接着统计r_max_w和r_max_h之间的最大值（宽度和高度方向差异最大的值）：
$$
r^{\max }=\max \left(r_{w}^{\max }, r_{h}^{\max }\right)
$$
4.如果GT Box和对应的Anchor Template的r_max小于阈值anchor_t（在源码中默认设置为4.0），即GT Box和对应的Anchor Template的高、宽比例相差不算太大，则将GT Box分配给该Anchor Template，示意图如下：

![在这里插入图片描述](https://img-blog.csdnimg.cn/95404cd4e7014fef9698b7b6f4fb5b90.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5aSq6Ziz6Iqx55qE5bCP57u_6LGG,size_20,color_FFFFFF,t_70,g_se,x_16)



5.将`GT`投影到对应预测特征层上，根据`GT`的中心点定位到对应`Cell`，因为网络预测中心点的偏移范围已经调整到了( − 0.5 , 1.5 ) ，所以按理说只要Grid Cell左上角点距离GT中心点在( − 0.5 , 1.5 ) 范围内它们对应的Anchor都能回归到GT的位置处，这样会让正样本的数量得到大量的扩充，YOLOv5源码中扩展`Cell`时只会往上、下、左、右四个方向扩展，下图中其中`%1`表示取余并保留小数部分；

![在这里插入图片描述](https://img-blog.csdnimg.cn/039b21ab9c014856b4dd5c8d5fab2f42.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5aSq6Ziz6Iqx55qE5bCP57u_6LGG,size_20,color_FFFFFF,t_70,g_se,x_16)

------



### YOLOv7



------



### ✅YOLO-X



**三点贡献：decoupled head / anchor-free / SimOTA**



**decoupled head：**

**YOLOX基于YOLOv5-5.0构建，因此Backbone和Neck相同，不同点在于YOLOv5的Head使用coupled detection head（使用1x1卷积同时预测cls，reg，objectness），YOLOX使用decoupled detection head；**

![在这里插入图片描述](https://img-blog.csdnimg.cn/08010b4889d5439c9721276e05e5f342.png#pic_center)



- `decoupled detection head`中对于预测`Cls`、`Reg`以及`IoU（objectness）`分别使用三个不同的分支，这样就将三者进行了解耦；

- YOLOX对于不同的预测特征图采用不同的head，即**参数不共享**；

![在这里插入图片描述](https://img-blog.csdnimg.cn/cc0dc356c7d4459aac95356c27516a58.png#pic_center)



**anchor-free：**

- 如上图`decoupled detection head`所示，它对预测feature map上的每一个位置都预测了`num_cls + 4 + 1`个参数，其中`num_cls`代表检测的目标类别，`4`代表网络预测的目标边界框，`1`代表object ness（图中标的是IoU.）;
- 由于YOLOX是Anchor-Free的网络，所以head在每个位置处直接预测4个目标边界框参数` [t_x,  t_y,  t_w,  t_h]`；
- 这4个参数分别对应预测目标中心点相对Grid Cell左上角`(c_x, c_y)`的偏移量，以及目标的宽度、高度因子，注意这些值都是相对预测特征图尺度上的，如果要映射回原图需要乘上当前特征图相对原图的步距stride；

```python
outputs[..., :2] = (outputs[..., :2] + grids) * strides  # 预测目标边界框中心坐标k
outputs[..., 2:4] = torch.exp(outputs[..., 2:4]) * strides  # 预测目标边界框宽度和高度
```

![在这里插入图片描述](https://img-blog.csdnimg.cn/35b76e8270dc4dd18e687967744a9e8f.png)



**损失计算：**

- **Classes loss**，采用的是`BCE loss`，只计算正样本；
- **Objectness loss(IoU)**，采用的依然是`BCE loss`，指的是网络预测的目标边界框与GT Box的`CIoU`，计算所有样本；
- **regression loss**，采用的是`IoU loss`，注意只计算正样本；
- `λ `代表定位损失的平衡系数，源码中设置是`5.0`；
- `N_pos`代表被分为正样的`Anchor Point`；

$$
L o s s=\frac{L_{c l s}+\lambda L_{r e g}+L_{o b j}}{N_{p o s}}
$$



**正负样本匹配策略SimOTA：**

- OTA简单来说就是将匹配正负样本的过程看成一个**最优传输问题**：如下图所示，假设有1到6共6个城市（图中的五角星），有2个牛奶生产基地A和B。现在要求这两个牛奶生产基地为这6个城市送牛奶，究竟怎样安排才能最小化运输成本。假设运输成本（cost）仅由距离决定，那么很明显城市1、2、3由牛奶生产基地A负责，城市4、5、6由牛奶生产基地B负责，运输成本最低；

![在这里插入图片描述](https://img-blog.csdnimg.cn/1236dd09ab864bdfa9432d1a0603db00.png)



- 在SimOTA正负样本匹配过程中，城市对应的是每个样本（对应论文中的anchor point，其实就是grid网格中的每个cell），牛奶生产基地对应的是标注好的GT Bbox，那现在的目标是怎样以最低的成本（cost）将GT分配给对应的样本。根据论文中的公式1，cost的计算公式如下，其中λ为平衡系数，代码中设置的是3.0，通过公式可以得知，成本cost由分类损失和回归损失两部分组成，并且网络预测的类别越准确cost越小，网络预测的目标边界框越准确cost越小。**那么最小化cost可以理解为让网络以最小的学习成本学习到有用的知识**；

$$
c_{i j}=L_{i j}^{c l s}+\lambda L_{i j}^{r e g}
$$

- **刚刚在上面有提到，城市对应的是每个样本，但不是所有的样本都要参与cost的计算：**FCOS将那些落入GT中心sub-box范围内的样本视为正样本，其他的都视为负样本；在`SimOTA`中也有个类似的预筛选过程：首先会将落入目标GT Bbox内或落入`fixed center area`内的样本给筛选出来，在源码中作者将`center_ratius`设置为`2.5`，如下图所示，feature map中所有打勾的位置都是通过预筛选得到的样本（`anchor point`）。注意，这里将落入GT Bbox与`fixed center area`相交区域内的样本用橙色的勾表示；



![在这里插入图片描述](https://img-blog.csdnimg.cn/c8e8fd2d4721462c9dd8aa6740e0b533.png)



- 接着计算网络在这些样本（anchor point）位置处的预测值（目标类别以及目标边界框）和每个GT的`L_cls`以及`L_reg`，由于回归损失是IoULoss，所以这里也知道每个样本和每个GT的IoU），然后再计算每个样本和每个GT之间的cost。这里需要注意下，在代码中计算cost的过程如下，和论文中给的公式有一点点区别：

```python
cost = (
            pair_wise_cls_loss
            + 3.0 * pair_wise_ious_loss
            + 100000.0 * (~is_in_boxes_and_center)  # is_in_boxes_and_center表示橙色的勾，～为取反操作，打黑色勾的会乘以这个系数导致cost特别大，这样在最小化cost过程中会优先选择GT Bbox与fixed center area交集内的样本
        )
```



**利用cost去进行正负样本的匹配：**

- 首先构建两个矩阵，一个是之前筛选出的Anchor Point与每个GT之间的cost矩阵，另一个是Anchor Point与每个GT之间的IoU矩阵，接着计算`n_candidate_k`并结合IoU对Anchor Point做进一步筛选（保留IoU大的Anchor Point）；
- `n_candidate_k`是取`10`和Anchor Point数量之间的最小值，在下面给的这个示例中由于Anchor Point数量为6，所以`n_candidate_k=6`故保留所有的Anchor Point；

![在这里插入图片描述](https://img-blog.csdnimg.cn/87598d38eb7c45baa42e4f6ff4b9d573.png)

- 接着对每个GT计算剩下所有的Anchor Point的IoU之和然后向下取整得到针对每个GT所采用的正样本数量，即代码中计算得到的`dynamic_ks`（这个计算过程对应论文中的Dynamic k Estimation Strategy）。对于下面的示例，GT1的所有Anchor Point的IoU之和为3.0向下取整就是3所以对于GT1有3个正样本，同理GT2也有3个正样本；

![在这里插入图片描述](https://img-blog.csdnimg.cn/e7a72f30f2714b14b0e3762c3c51d345.png)



- 然后根据刚刚计算得到的dynamic_ks（每个GT对应几个正样本）和cost矩阵找出所有的正样本（根据cost的数值大小）。比如对于示例中的GT1，刚刚计算采用3个正样本，然后看下GT1和所有Anchor Point的cost，按照从小到大的顺序将前3小的Anchor Point找出来，即示例中的A1、A2和A5。同理对于GT2，cost排前3的是A3、A4和A5。根据以上结果，我们可以再构建一个Anchor Point分配矩阵，记录每个GT对应哪些正样本，对应正样本的位置标1，其他位置标0；

![在这里插入图片描述](https://img-blog.csdnimg.cn/13b9130133e243b6ad1c64eb528dfce5.png)



- 按照示例我们会发现一个问题，即GT1和GT2同时分配给了`A5`。作者为了解决这个带有歧义的问题，又加了一个判断。如果多个GT同时分配给一个Anchor Point，那么只选cost最小的GT。在示例中，由于`A5`与GT2的cost小于与GT1的cost，故只将GT2分配给`A5`;

![在这里插入图片描述](https://img-blog.csdnimg.cn/990a25935b1541b28548b84e7bdf6dfe.png)



------



## #####################################################

## Anchor-Free



### ✅FCOS

**跳出Anchor的限制，使用语义分割的思想逐像素来解决检测问题：**在预测特征图的每个位置上直接去预测该点分别距离目标左侧（l: left），上侧（t：top），右侧(r: right)以及下侧（b：bottom）的距离，如下图所示：

![在这里插入图片描述](https://img-blog.csdnimg.cn/b86c718e9915482c8de126db0286133a.png)



**网络结构：**

![在这里插入图片描述](https://img-blog.csdnimg.cn/6802bfa22b4e49669f0bbb22c6a6eb98.png)

![在这里插入图片描述](https://img-blog.csdnimg.cn/1221dcc28ee44894a3371d4b58b411e6.png#pic_center)



**输出：**

- **Classification**：在预测特征图的每个位置上都会预测C个score参数；【类别】
- **Regression**：在预测特征图的每个位置上都会预测4个距离参数l，r，t，b【预测的数值是相对特征图尺度】，假设对于预测特征图上某个点映射回原图的坐标是(C_x, C_y)，特征图相对原图的步距是`s`，那么网络预测该点对应的目标边界框坐标为：

$$
\begin{aligned}
x_{\min }=c_{x}-l \cdot s, y_{\min } &=c_{y}-t \cdot s \\
x_{\max } &=c_{x}+r \cdot s, y_{\max }=c_{y}+b \cdot s
\end{aligned}
$$

- **Center-ness：**预测特征图的每个位置上都会预测1个参数，`center-ness`反映的是该点（特征图上的某一点）距离目标中心的远近程度，它的值域在0~1之间，距离目标中心越近`center-ness`越接近于1，下面是`center-ness`真实标签的计算公式（计算损失时只考虑正样本，即预测点在目标内的情况）

$$
\text { centerness }^{*}=\sqrt{\frac{\min \left(l^{*}, r^{*}\right)}{\max \left(l^{*}, r^{*}\right)} \times \frac{\min \left(t^{*}, b^{*}\right)}{\max \left(t^{*}, b^{*}\right)}}
$$

​    在网络后处理部分筛选高质量bbox时，会将预测的目标`class score`与`center-ness`相乘再开根，然后根据得到的结果对bbox进行排序，只保留分数较高的bbox，这样做的目的是筛掉那些目标`class score`低且预测点距离目标中心较远的bbox，最终保留下来的就是高质量的bbox；



**正负样本的匹配：**

- 对于特征图上的某一点(x, y)，只要它落入GT box中心区域，那么它就被视为正样本；
- (cx , cy ) 在 (cx − rs, cy − rs, cx + rs, cy + rs)这个sub-box范围之内，其中(cx , cy ) 是GT的中心点，`s`是特征图相对原图的步距，`r`是一个超参数控制距离GT中心的远近；
- 换句话说点(x , y ) 不仅要在GT的范围内，还要离GT的中心点(cx , cy ) 足够近才能被视为正样本；



假设上面两个feature map对应的是同一个特征图，将特征图上的每个点映射回原图就是下面图片中黑色的圆点。根据2019年发表论文的匹配准则，只要落入GT box就算正样本，所以左侧的feature map中打勾的位置都被视为正样本。根据2020年的版本，不仅要落入GT Box还要在(cx − rs, cy − rs, cx + rs, cy + rs)这个`sub-box`范围内，所以右侧的feature map中打勾的位置都被视为正样本；



![在这里插入图片描述](https://img-blog.csdnimg.cn/c015759cb6194d20bfa16c3a34d2861d.png)



**损失计算：**

损失由分类损失L_cls、定位损失L_reg以及`center-ness`损失L_ctrness三部分共同组成：

- 分类损失L_cls采用`bce_focal_loss`，即二值交叉熵损失配合`focal_loss`，计算损失时所有样本都会参与计算（正样本和负样本）;
- 定位损失L_reg采用`giou_loss`,计算损失时只有正样本参与计算;
- `center-ness`损失L_ctrness采用二值交叉熵损失，计算损失时只有正样本参与计算;


$$
\begin{aligned}
L\left(\left\{p_{x, y}\right\},\left\{t_{x, y}\right\},\left\{s_{x, y}\right\}\right) &=\frac{1}{N_{p o s}} \sum_{x, y} L_{c l s}\left(p_{x, y}, c_{x, y}^{*}\right) \\
&+\frac{1}{N_{p o s}} \sum_{x, y} 1_{\left\{c_{x, y}^{*}>0\right\}} L_{r e g}\left(t_{x, y}, t_{x, y}^{*}\right) \\
&+\frac{1}{N_{p o s}} \sum_{x, y} 1_{\left\{c_{x, y}^{*}>0\right\}} L_{c t r n e s s}\left(s_{x, y}, s_{x, y}^{*}\right)
\end{aligned}
$$

- p_(x,y)表示在特征图点(x,y)处预测的每个类别的score,c*_(x,y)表示在特征图点(x,y)对应的真实类别标签;
- l{c*_(x,y)>0}当特征图(x,y)点被匹配为正样本时为1，否则为0;
- t(x,y)表示在特征图点(x,y)处预测的目标边界框信息,t*_(x,y)表示在特征图点(x,y)对应的真实目标边界框信息;
- S(x,y)表示在特征图点(x,y)处预测的`center-ness`,S(x,y)表示在特征图点(x,y)对应的真实`center-ness`;



假设对于特征图上的某一个点（图中用蓝色填充的`cell`）映射回原图，对应图片中的黑色点。然后计算该点距离GT box左侧，上侧，右侧，下侧的距离就能得到`l*，r*，t*，b*`，再套用上面的公式就能得到s*(x,y)；

![在这里插入图片描述](https://img-blog.csdnimg.cn/7a9bed45b8a14ec098327de352c3c4c8.png)



**Ambiguity问题：（如果feature map上的某个点同时落入两个GT Box相交区域）**

- 默认将该点分配给**面积Area最小的GT Box**【球拍】；
- FPN：FPN中会采用多个预测特征图，不同尺度的特征图负责预测不同尺度的目标：比如P3负责预测小型目标，P5负责预测中等目标，P7负责预测大型目标，这样在匹配正负样本时能够将部分重叠在一起的目标（这里主要指不同尺度的目标）给分开；
- **center sampling匹配准则**：匹配正样本时要求不仅要落入GT Box还要在 (cx − rs, cy − rs, cx + rs, cy + rs)这个`sub-box`范围内；



**FPN每个特征层范围正负样本划分：**More specifically, we first compute the regression targets l ∗ , t ∗ , r ∗ and b ∗ for each location on all feature levels. Next, if a location at feature level i satisfies max(l ∗ , t ∗ , r∗ , b∗ ) ≤ mi−1 or max(l ∗ , t∗ , r∗ , b∗ ) ≥ mi , it is set as a negative sample and thus not required to regress a bounding box anymore. Here mi is the maximum distance that feature level i needs to regress. In this work, m2, m3, m4, m5, m6 and m7 are set as 0, 64, 128, 256, 512 and ∞, respectively.



![在这里插入图片描述](https://img-blog.csdnimg.cn/ec85831cb2f14fbbbf6bc83866fcd4de.png)



**改进**

```
所有正样本点的 weight 平权 -> 将样本点对应的 centerness 作为权重，离 GT 中心越近，权重越大
centerness 分支利用 l，t，r，b 计算 centerness -> 用 IoU Style 
```



### CornerNet



### CenterNet



------

## #####################################################

## Transformer-Style



### DETR
